# -*- coding: utf-8 -*-
"""PUSULA_SEMIH_ASDAN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YcxYUoHb0_un5Ub0v9c1L4dzyLL25J45

##Imports
"""

!pip install thefuzz python-levenshtein

import pandas as pd
from thefuzz import fuzz
from itertools import combinations
import numpy as np
from collections import Counter
import warnings
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import re

"""# Exploratory Data Analysis (EDA):

## Text-Oriented Data Analysis
"""

sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = [12, 7]

file_path = 'Talent_Academy_Case_DT_2025.xlsx'
df = pd.read_excel(file_path)
display(df.head()), df.info(), display(df.describe().T), df.columns

missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
missing_info = pd.DataFrame({
  'missing values': missing_values,
  'missing values %': missing_percentage
}).sort_values(by='missing values %', ascending=False)
display(missing_info[missing_info['missing values'] > 0])

#creating a comprehensive function to easily track the latest status of the data during data cleaning and processing.
def generate_comprehensive_report(df):
    print(f"rows x cols: {df.shape}")
    print(f"Total cells: {df.size}")
    print(f"Duplicated rows: {df.duplicated().sum()}")
    print("Column types:")
    print(df.dtypes)

    print("\n missing")
    missing = df.isnull().sum()
    missing_percent = (missing / len(df)) * 100
    missing_info = pd.DataFrame({
        'Missing Count': missing,
        'Missing %': missing_percent
    }).sort_values(by='Missing Count', ascending=False)
    print(missing_info[missing_info['Missing Count'] > 0])

    num_cols = df.select_dtypes(include=np.number).columns
    print(df[num_cols].describe().T)

    print("\n category")
    cat_cols = df.select_dtypes(include=['object', 'category']).columns
    complex_cols = ['Tanilar', 'TedaviAdi', 'KronikHastalik', 'Alerji', 'UygulamaYerleri']
    simple_cols = [c for c in cat_cols if c not in complex_cols]

    for col in simple_cols:
        print(f"\nColumn: {col}")
        counts = df[col].value_counts()
        print(f"Unique values: {len(counts)}")
        print(counts.head(10))

    def analyze_split_column(df, col, top_n=10):
        print(f"\nColumn: {col}")
        items = df[col].dropna().str.split(',').explode().str.strip()
        items = items[items != '']
        if items.empty:
            print("No valid data.")
            return
        counts = Counter(items)
        print(f"Unique items: {len(counts)}")
        for item, count in counts.most_common(top_n):
            print(f"{item:<30} | {count}")

    for col in complex_cols:
        if col in df.columns:
            analyze_split_column(df, col)

generate_comprehensive_report(df)

"""## Visual-Oriented Data Analysis"""

msno.matrix(df)
plt.title('Matrix Visualization of Missing Data', fontsize=16)
plt.show()
msno.heatmap(df)
plt.title('Missing Data Heatmap', fontsize=16)
plt.show()

numerical_cols = ['Yas', 'TedaviSuresi', 'UygulamaSuresi']

for col in numerical_cols:
  plt.figure(figsize=(24, 6))
  # Histogram: Değişkenin dağılımını gösterir
  plt.subplot(1, 2, 1)
  sns.histplot(df[col], kde=True, bins=30)
  plt.title(f'{col} Histogram', fontsize=14)
  # Boxplot: Aykırı değerleri ve çeyrekleri gösterir
  plt.subplot(1, 2, 2)
  sns.boxplot(x=df[col])
  plt.title(f'{col} Boxplot', fontsize=14)
  plt.suptitle({col.upper()}, fontsize=16, y=1.02)
  plt.tight_layout()
  plt.show()

categorical_cols = ['Cinsiyet', 'KanGrubu', 'Bolum']

for col in categorical_cols:
  plt.figure(figsize=(10, 6))
  ax = sns.countplot(y=df[col], order=df[col].value_counts().index, palette='viridis', hue=df[col], legend=False)
  plt.title(f'{col}', fontsize=16)
  plt.xlabel('Frekans', fontsize=12)
  plt.ylabel(col, fontsize=12)
  for p in ax.patches:
    width = p.get_width()
    plt.text(width + 0.1, p.get_y() + p.get_height()/2.,
      f'{int(width)}',
      ha='left', va='center')

plt.show()

sns.scatterplot(data=df, x='Yas', y='TedaviSuresi', alpha=0.6)
plt.title('Relationship Between Age and Treatment Duration', fontsize=16)
plt.show()

"""## Findings
1. `HastaNo` gibi bir sonucu tahmin etmek için hiçbir anlamlı desen içermeyen, model için saf bir gürültü kaynağıdır. Bu nedenle modelin yanlış ezber yapmasını engellemek için **silinecek**.

2. `Uyruk` sütundaki verilerin neredeyse tamamını "Türkiye" oluşmaktadır. Modelin farklı sonuçları ayırt etmesine yardımcı olacak anlamlı bir çeşitlilik (varyans) sunmadığı için tahmin gücü yoktur ve gereksiz bir özelliktir **silinecek**.

3. **Çok kirli veri güzel bir temizliği ihtiyaç var** `Alerji` sütununda `Polen` ve `POLEN` şeklinde aynı bilgi 2 farklı şekilde yazılmış. `Toz`, `TOZ` ve `Toz,TOZ` değerleri var belli satırlarda. Diğer sütunlardada yazım farklılıkları, yazım hataları, farklı ek kullanımı(kolla ve kol ile gibi) ve gerkesiz numaralandırmalar oldukça fazla.

4. Hedef veri olan `TedaviSuresi` sütununda 15 seans oranı çok fazla (+1500).
 `KisaSureliTedavi`, `OrtaSureliTedavi`, `UzunSureliTedavi` gibi kategorilere ayrılabilir.  

5. `Bolum` sütununda `Fiziksel Tıp Ve Rehabilitasyon` oranı aşırı fazla(+2000) ama kıymetli bir veri olduğu için silmek yerine çok nadir görünen bölümleri (örneğin, toplamda 5'ten az görünenler) "Diğer_Bolumler" adında tek bir kategori altında toplanabilir. Bu, kategorik değişkenin karmaşıklığını (cardinality) azaltır ve modelin daha iyi genelleme yapmasına yardımcı olur.

6. `UygulamaSuresi` sütununda verilerin büyük kısmı(+1500) "20 Dakika" değerine sahip. Bu durumun diğer verilerle ilişkisini daha iyi anlamak için `UygulamaSuresi` 20 dk mı değil mi(1-0) sütünu oluşturulabilir.

7. **Tamamen aynı veriler kopyalanarak çoğaltılmış.** `HastaNo`'dan `UygulamaYerleri`'ne kadar tüm veriler birebir aynı. Mesela veriden 4 adet oluşturulmuş 4 veriden 1'inin UygulamaSuresi verisi değiştirilmiş.
Bunlar  UygulamaSuresi ortalması alınarak birleştirilecek.

8. Bu kopya veriler eksik verileri doldurmkata işe yarar gözüküyor
Örnek: Aynı `HastaNo` ama `KanGrubu`, `KronikHastalık`, `Cinsiyet`, `Alerji` gibi verileri bir satırda var diğerinde yok.

9. `TedaviAdi` verisi oldukça fazla `UygulamaYerleri` verisi içeriyor kendi içinde. Bu hem `UygulamaYerleri` verisini doldurmak için bir fırsta hem de `TedaviAdi` verisi için gerksiz bir gürültü. Aynı şekilde `Tani` veriside bolca `UygulamaYerleri` verisi içersede bazı durumlarda baş ağrısının Uygulama yeri sırt olaibliryor. Bu nedenle en sağlıklısı `TedaviAdi`-> `UygulamaYerleri` veri doldurma.

**Not:** Burada tespitler doğru ama düşünülen çözümlerin bazıları ilerleyen aşamada daha detaylı incelemelerle değişecektir.

**Eksikler**
1. `Alerji`: %42(944)

2. `KanGrubu`: %30(675)

3. `KronikHastalık`: %27(611)

4. `UygulamaYerleri`: %10(221)

5. `Cinsiyet`: %7(169)

6. `Tanılar`: %3(75)

# Data Processing

## Cleaning
"""

df.drop(columns='Uyruk', inplace=True)

columns_to_clean = [
    'Cinsiyet', 'KanGrubu', 'KronikHastalik', 'Bolum',
    'Alerji', 'Tanilar', 'TedaviAdi', 'TedaviSuresi',
    'UygulamaYerleri', 'UygulamaSuresi'
]
# 'i̇'
def turkish_lower(text):
    if not isinstance(text, str):
        return text
    text = text.replace('I', 'ı')
    text = text.replace('İ', 'i')
    text = text.lower()
    return text

for col in columns_to_clean:
  if df[col].dtype == 'object':
     df[col] = df[col].str.strip()
     df[col] = df[col].apply(turkish_lower)


def advanced_text_cleaner(text):

    if not isinstance(text, str):
        return text

    garbage_pattern = r'[\u200b\u200c\u200d\ufeff\xad`´\'\"]|_x000d_'
    cleaned_text = re.sub(garbage_pattern, '', text)
    # çoklu boşluklar
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text

columns_for_regex_cleaning = [
    'KronikHastalik', 'Bolum', 'Alerji', 'Tanilar',
    'TedaviAdi', 'UygulamaYerleri'
]
for col in columns_for_regex_cleaning:
  if col in df.columns:
    df[col] = df[col].apply(advanced_text_cleaner)


output_file = 'Talent_Clean.csv'
df.to_csv(output_file, index=False, encoding='utf-8-sig')

print("Temizlik işlemi tamamlandı.")

df['TedaviSuresi'] = df['TedaviSuresi'].str.replace(' seans', '', regex=False).str.strip()
df['UygulamaSuresi'] = df['UygulamaSuresi'].str.replace(' dakika', '', regex=False).str.strip()
df['TedaviSuresi'] = pd.to_numeric(df['TedaviSuresi'], errors='coerce')
df['UygulamaSuresi'] = pd.to_numeric(df['UygulamaSuresi'], errors='coerce')
df.to_csv('Talent_Clean.csv', index=False)
print("dakika ve seans yazıları silindi sütunlar sayısal değere dönüştürüldü.")

def find_similar_strings(dataframe, column_name, threshold=85):

    print(f"\n'{column_name}' Text Similarity Analysis")

    series = dataframe[column_name].dropna().astype(str).str.split(',').explode()
    unique_values = series.str.strip().unique()

    groups = defaultdict(list)

    processed = set()

    for value in unique_values:
        if value in processed:
            continue

        current_group = [value]
        processed.add(value)

        for other_value in unique_values:
            if other_value in processed:
                continue

            ratio = fuzz.ratio(value.lower(), other_value.lower())

            if ratio >= threshold:
                current_group.append(other_value)
                processed.add(other_value)

        if len(current_group) > 1:
            print(f"Potansiyel Grup Bulundu: {current_group}")

find_similar_strings(df, 'Alerji', threshold=85)
find_similar_strings(df, 'Tanilar', threshold=85)
find_similar_strings(df, 'TedaviAdi', threshold=85)
find_similar_strings(df, 'UygulamaYerleri', threshold=85)
find_similar_strings(df, 'KronikHastalik', threshold=35)
find_similar_strings(df, 'Bolum', threshold=35)

"""### Yazım Hataları Tespitleri

 * volteren, voltaren → Voltaren

 * hiportiroidizm, hipotirodizm → Hipotiroidizm - Hipertiroidizm farklı ve doğru yazılmış

1. Noktalı 'i'
 * dorsalji̇, dorsalji 1 → dorsalji
 * di̇ğer → diğer
 * gri̇pi̇n → gripin
 * sol kalça i̇mplanti → sol kalça implantı
 * sol hemi̇pleji̇ → sol hemipleji
 * meni̇sküs tami̇ri̇ → menisküs tamiri
 * i̇ç hastalıkları → iç hastalıkları
 * tanimlanmamiş → tanımlanmamış (Ayrıca tanımlanmış ifadesi de tanımlanmamış ile standartlaştırılmalı)
 * bi̇rden fazla yer → birden fazla yer
 * omuz bölgesi̇ → omuz bölgesi
 * hipotirodizm → hiportiroidizm (Tıbbi olarak doğru yazım)
2. Gereksiz veya Tutarsız Ekler
 * radikülopati ile → radikülopati
 * serebrovasküler hastalıklar diğer → serebrovasküler hastalıklar
 * yer tanımlanmamış → tanımlanmamış
 * eklem ağrsıı → eklem ağrısı
 * eklemin kontraktürü → eklem kontraktürü
 * sol omuz impingement (darbe sendromu) → sol omuz impingement
3. Kısaltma ve Numaralandırma Tutarsızlıkları
 * aşil rüptürü-1, aşil rüptürü-2, aşil rüptürü op → Hepsi aşil   rüptürü ana başlığı altında toplanabilir.
 * koksartroz1, koksartroz 1, koksartroz1-1, koksartroz1-2 → Hepsi koksartroz olarak birleştirilmeli.
 * epikondilit-1, epikondilit-2 → epikondilit olarak birleştirilmeli.
 * malleol kırığı-1, malleol kırığı-2 → malleol kırığı olarak birleştirilmeli.
 * tendinit-tenosinovit-1, tendinit-tenosinovit-2 → tendinit-tenosinovit olarak birleştirilmeli.
 * i̇v di̇sk bozukluğu-bel-1, i̇v di̇sk bozukluğu-bel-2 → iv disk bozukluğu-bel olarak birleştirilmeli.
 * tfcc rehabilitasyonu, tfcc rehabilitasyon → tfcc rehabilitasyonu olarak standartlaştırılmalı.
 * gonartroz-meniskopati → gonartroz, meniskopati
 * alt ekstremite atrofi-bilateral ve alt ekstremite atrofi+yürüme → alt ekstremite atrofi
 * el rehabilitasyonu-el rehabilitasyonu → el rehabilitasyonu
 * kondromalezi patella-1, kondromalezi patella-2 → kondromalezi patella
 * sol öçb rehabilitasyonu-1, sol öçb rehabilitasyonu-2 → sol öçb rehabilitasyonu
4. Sadece `Tedavi Adı` sütunu için(çünkü sürekli burda belirtilen bel, ayak, omuz gibi lokasyonlar zaten `UygulamaYeri` sütunununda var)
 * dorsalji -boyun+trapez, dorsalji-bel, dorsalji-dorsal vb. → dorsalji
 * sol omuz impingement, impingement sağ, i̇mpimgement send, sağ omuz impingement → Bunların hepsi "impingement"
 * En iyisi tüm lokasyon belirten kelimeleri sütunda kaldırmak.
 * Hatta `UygulamaYeri` sütununda %10 boş olan verinin bir kısmını bu sütundaki lokayson verisiyle doldurabiliriz
 * Parapleji(sadece parapleji yazıyorsa) → belden aşağı (Parapleji sadece Belden aşağı uygulanan bir tedaviymiş)
 * lenfödem ekstremite ağrısı → kol veya bacak
 * alt ekstremite atrofi → bacak
"""

# 1. Alerji Column
alerji_corrections = {
    r'volteren': 'voltaren',
    r'novalgın': 'novalgin',
    r'grıpın': 'gripin'
}

# 2. KronikHastalik Column
kronik_hastalik_corrections = {
    r'hipotirodizm': 'hiportiroidizm',
}

# 3. Tanilar Column
tanilar_corrections = {
    r'\btanimlanmamiş\b': 'tanımlanmamış',
    r'\btanımlanmış\b': 'tanımlanmamış',
    r'radikülopati ile': 'radikülopati',
    r'serebrovasküler hastalıklar diğer': 'serebrovasküler hastalıklar',
    r'\bserebrovasküler hastalık\b': 'serebrovasküler hastalıklar',
    r'\byer tanımlanmamış\b': 'tanımlanmamış',
    r'eklem ağrsıı': 'eklem ağrısı',
    r'eklem kontraktürü': 'eklemin kontraktürü',
    r'yeri tanımlanmamış': 'tanımlanmamış',
    r'diğer tanımlanmamış': 'tanımlanmamış',
    r'pelvık bölge ve kalça': 'pelvik bölge ve kalça',
}

# 4. TedaviAdi Column
tedavi_adi_corrections = {
    r'aşil rüptürü-\d+': 'aşil rüptürü',
    r'aşil rüptürü op': 'aşil rüptürü',
    r'koksartroz\s?\d+(-\d+)?': 'koksartroz',
    r'epikondilit-\d+': 'epikondilit',
    r'malleol kırığı-\d+': 'malleol kırığı',
    r'tendinit-tenosinovit-\d+': 'tendinit-tenosinovit',
    r'iv disk bozukluğu bel-\d+': 'iv disk bozukluğu bel',
    r'iv disk bozukluğu bel': 'iv disk bozukluğu bel',
    r'iv disk bozukluğu bel 1': 'iv disk bozukluğu bel',
    r'iv disk bozukluğu bel 2': 'iv disk bozukluğu bel',
    r'iv disk bozukluğu 1': 'iv disk bozukluğu',
    r'iv disk bozukluğu 2': 'iv disk bozukluğu',
    r'dorsalji 1': 'dorsalji',
    r'kondromalezi patella-\d+': 'kondromalezi patella',
    r'sol öçb rehabilitasyonu-\d+': 'sol öçb rehabilitasyonu',
    r'tfcc rehabilitasyon\b': 'tfcc rehabilitasyonu',
    r'el rehabilitasyonu-el rehabilitasyonu': 'el rehabilitasyonu',
    r'gonartroz-meniskopati': 'gonartroz, meniskopati',
    r'alt ekstremite atrofi-bilateral': 'alt ekstremite atrofi',
    r'alt ekstremite atrofi\+yürüme': 'alt ekstremite atrofi',
    r'impingiment': 'impingement',
    r'impingemen': 'impingement',
    r'kondromalazi patella': 'kondromalezi patella',
    r'muskuler strain': 'muscular strain',
    r'eklem ağrsıı': 'eklem ağrısı',
    r'impingementt': 'impingement',
}

print("Sadeleştirilmiş kurallarla temizlik işlemi başlıyor...")

for wrong, correct in alerji_corrections.items():
    df['Alerji'] = df['Alerji'].str.replace(wrong, correct, regex=False)

for wrong, correct in kronik_hastalik_corrections.items():
    df['KronikHastalik'] = df['KronikHastalik'].str.replace(wrong, correct, regex=False)

for wrong_pattern, correct_text in tanilar_corrections.items():
    df['Tanilar'] = df['Tanilar'].str.replace(wrong_pattern, correct_text, regex=True)

for wrong_pattern, correct_text in tedavi_adi_corrections.items():
    df['TedaviAdi'] = df['TedaviAdi'].str.replace(wrong_pattern, correct_text, regex=True)

df['TedaviAdi'] = df['TedaviAdi'].str.replace(r'[\-\+]', ' ', regex=True).str.strip()
df['TedaviAdi'] = df['TedaviAdi'].str.replace(r'\s+', ' ', regex=True)

print("Düzenleme işlemi tamamlandı.")

find_similar_strings(df, 'Alerji', threshold=85)
find_similar_strings(df, 'Tanilar', threshold=85)
find_similar_strings(df, 'TedaviAdi', threshold=85)
find_similar_strings(df, 'UygulamaYerleri', threshold=85)
find_similar_strings(df, 'KronikHastalik', threshold=35)
find_similar_strings(df, 'Bolum', threshold=35)
# 'sol omuz impingement', 'sağ omuz impingement' not change them because they are semantically different.

"""## Processing

Finding 9

9. `TedaviAdi` verisi oldukça fazla `UygulamaYerleri` verisi içeriyor kendi içinde. Bu hem `UygulamaYerleri` verisini doldurmak için bir fırsta hem de `TedaviAdi` verisi için gerksiz bir gürültü. Aynı şekilde `Tani` veriside bolca `UygulamaYerleri` verisi içersede bazı durumlarda baş ağrısının Uygulama yeri sırt olaibliryor. Bu nedenle en sağlıklısı `TedaviAdi`-> `UygulamaYerleri` veri doldurma.
"""

missing_counts = df.isnull().sum()
missing_percentage = (missing_counts / len(df)) * 100
print(missing_counts, missing_percentage)

# AI prepared this list for me
location_keywords = sorted([
    'sol ayak bileği', 'sağ ayak bileği', 'sol el bileği', 'sağ el bileği',
    'sol omuz', 'sağ omuz', 'sol kalça', 'sağ kalça', 'sol diz', 'sağ diz',
    'sol dirsek', 'fas','sağ dirsek', 'tüm vücut', 'el bilek',
    'bilateral diz', 'bilateral', 'servikal', 'dorsal', 'lomber',
    'omuz', 'diz', 'kalça', 'boyun', 'bel', 'sırt', 'ayak bileği',
    'el bileği', 'dirsek', 'el', 'parmak', 'yüz', 'trapez', 'skapular'
], key=len, reverse=True)

location_pattern = '|'.join(location_keywords)
cleaning_pattern = r'\b(' + '|'.join(location_keywords) + r')\b'

def process_treatment_and_location(row):
    tedavi_adi = row['TedaviAdi']
    uygulama_yeri = row['UygulamaYerleri']

    new_tedavi_adi = tedavi_adi
    new_uygulama_yeri = uygulama_yeri

    if pd.isna(uygulama_yeri) and pd.notna(tedavi_adi):
        # Data to be filled as a result of my own research
        if 'sağ subtrokanterik kırık' in tedavi_adi:
            new_uygulama_yeri = 'sağ kalça/femur'
        elif 'sağ sol humerus kırığı' in tedavi_adi:
            new_uygulama_yeri = 'sağ ve sol kol'
        elif 'humerus kırığı' in tedavi_adi:
            new_uygulama_yeri = 'kol'
        elif 'gonartroz meniskopati kalkaneal spur' in tedavi_adi:
            new_uygulama_yeri = 'diz'
        elif 'menisektomi erken rehabilitasyon' in tedavi_adi or 'serebrovasküler olay' in tedavi_adi or 'organ nakli mobilizasyon' in tedavi_adi or 'serebral palsi' in tedavi_adi:
            new_uygulama_yeri = 'tüm vücut bölgesi'
        elif 'paraparezi' in tedavi_adi:
            new_uygulama_yeri = 'bacaklar/alt vücut'
        elif 'sağ medial epikondilit' in tedavi_adi:
            new_uygulama_yeri = 'sağ dirsek'
        elif 'impingement sağ' in tedavi_adi:
            new_uygulama_yeri = 'sağ omuz'
        elif 'sol öçb rehabilitasyonu' in tedavi_adi:
            new_uygulama_yeri = 'sol diz'
        elif 'kalkaneal spur' in tedavi_adi:
            new_uygulama_yeri = 'topuk'
        elif 'parapleji' in tedavi_adi and 'tetrapleji' not in tedavi_adi:
            new_uygulama_yeri = 'belden aşağı'
        elif 'lenfödem' in tedavi_adi:
            new_uygulama_yeri = 'kol veya bacak'
        elif 'alt ekstremite atrofi' in tedavi_adi:
            new_uygulama_yeri = 'bacak'
        else:
            match = re.search(location_pattern, tedavi_adi, re.IGNORECASE)
            if match:
                found_location = match.group(0).strip()
                new_uygulama_yeri = found_location

    if pd.notna(new_tedavi_adi):
        cleaned_adi = re.sub(cleaning_pattern, '', new_tedavi_adi, flags=re.IGNORECASE)

        cleaned_adi = re.sub(r'[\-\+\.,]', ' ', cleaned_adi)
        cleaned_adi = re.sub(r'\s+', ' ', cleaned_adi).strip()

        if cleaned_adi == 'rehabilitasyonu':
            cleaned_adi = 'rehabilitasyon'
        # Clean text with 2 or fewer characters
        if len(cleaned_adi) <= 2:
            cleaned_adi = ''

        new_tedavi_adi = cleaned_adi if cleaned_adi else np.nan

    return new_tedavi_adi, new_uygulama_yeri

df[['TedaviAdi_temiz', 'UygulamaYerleri_dolu']] = df.apply(
    process_treatment_and_location,
    axis=1,
    result_type='expand'
)

df['TedaviAdi'] = df['TedaviAdi_temiz']
df['UygulamaYerleri'] = df['UygulamaYerleri_dolu']
df = df.drop(columns=['TedaviAdi_temiz', 'UygulamaYerleri_dolu'])
file_id = '1T9U6NGdgfde5LYcthu8JQTJ_WCcwS-lG'

missing_counts = df.isnull().sum()
missing_percentage = (missing_counts / len(df)) * 100
# Missing Data => UygulamaYerleri: %10(221) -> %3(68)
print(missing_counts, missing_percentage)
url = f'https://drive.google.com/uc?export=download&id={file_id}'
df = pd.read_csv(url) # nerdeyse tamamen aynı dosya
df.to_csv(output_file, index=False, encoding='utf-8-sig')

"""Finding 7

7. **Tamamen aynı veriler kopyalanarak çoğaltılmış.** `HastaNo`'dan `UygulamaYerleri`'ne kadar tüm veriler birebir aynı. Mesela veriden 4 adet oluşturulmuş 4 veriden 1'inin UygulamaSuresi verisi değiştirilmiş.
Bunlar  UygulamaSuresi ortalması alınarak birleştirilecek.
"""

initial_row_count = len(df)
print(f"Total number of rows at the beginning: {initial_row_count}")

print("\n Ex. (HastaNo: 145135):")

display_cols = ['HastaNo', 'TedaviAdi', 'TedaviSuresi', 'UygulamaYerleri', 'UygulamaSuresi']
display(df.loc[df['HastaNo'] == 145135, display_cols])

# All other columns except 'UygulamaSuresi'
grouping_columns = [
    'HastaNo', 'Yas', 'Cinsiyet', 'KanGrubu', 'KronikHastalik',
    'Bolum', 'Alerji', 'Tanilar', 'TedaviAdi', 'TedaviSuresi', 'UygulamaYerleri'
]

print(f"{len(grouping_columns)} columns will be used for grouping: {grouping_columns}")
# groupby().agg()

df_aggregated = df.groupby(grouping_columns, as_index=False, dropna=False).agg(
    UygulamaSuresi=('UygulamaSuresi', 'mean')
)
df_aggregated['UygulamaSuresi'] = df_aggregated['UygulamaSuresi'].round().astype(int)
# values ​​were rounded and converted to integers

final_row_count = len(df_aggregated)
print(f"Total number of rows at the after: {final_row_count}")
print(f"Total {initial_row_count - final_row_count} row merged.")
print("\n Ex. (HastaNo: 145135):")
display(df_aggregated.loc[df_aggregated['HastaNo'] == 145135, display_cols])

display(df_aggregated.head())

"""The `Alerji` data is very incomplete. I tried filling it in by referencing the `HastaNo`, but it didn’t work. In fact, in daily practice, it’s probably left blank because the patient doesn’t have any allergies, so the doctor didn’t write anything."""

df_processed = df_aggregated.copy()

missing_before = df_processed['Alerji'].isnull().sum()
cols_before = df_processed.shape[1]
print(f"missing_before: {missing_before}")
print(f"cols_before: {cols_before}")

df_processed['Alerji'].fillna('Alerji_Yok', inplace=True)

# Adım 2: ONE-HOT ENCODING (get_dummies ile)
# 'str.get_dummies()
alerji_dummies = df_processed['Alerji'].str.get_dummies(sep=',')

alerji_dummies.columns = [col.strip() for col in alerji_dummies.columns]

alerji_dummies = alerji_dummies.add_prefix('Alerji_')
print(f"Adım 2: {len(alerji_dummies.columns)} adet yeni alerji sütunu oluşturuldu.")


df_processed = pd.concat([df_processed, alerji_dummies], axis=1)


df_processed.drop('Alerji', axis=1, inplace=True)

cols_after = df_processed.shape[1]
print(f"cols_after: {cols_after} (Increase: {cols_after - cols_before + 1})")

new_allergy_cols = [col for col in df_processed.columns if col.startswith('Alerji_')]
print(f"\n new cols: {new_allergy_cols}")

display(df_processed[['HastaNo'] + new_allergy_cols].head(10))

# Bir Tanının Tedavi adı ve bir tedavi adının tanısı arasında çok benzerlik olacağı için birbirlerine referans olacak şekilde karşılıklı eksik veri doldurma
missing_tani_before = df_processed['Tanilar'].isnull().sum()
missing_tedavi_before = df_processed['TedaviAdi'].isnull().sum()
print(f"missing_tani_before={missing_tani_before}, missing_tedavi_before={missing_tedavi_before}")


tani_to_tedavi_map = df_processed.groupby('Tanilar')['TedaviAdi'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).to_dict()

tedavi_to_tani_map = df_processed.groupby('TedaviAdi')['Tanilar'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).to_dict()

tedavi_missing_idx = df_processed[df_processed['TedaviAdi'].isnull()].index
tedavi_imputed_values = df_processed.loc[tedavi_missing_idx, 'Tanilar'].map(tani_to_tedavi_map)
df_processed.loc[tedavi_missing_idx, 'TedaviAdi'] = tedavi_imputed_values

tani_missing_idx = df_processed[df_processed['Tanilar'].isnull()].index
tani_imputed_values = df_processed.loc[tani_missing_idx, 'TedaviAdi'].map(tedavi_to_tani_map)
df_processed.loc[tani_missing_idx, 'Tanilar'] = tani_imputed_values

missing_tani_after = df_processed['Tanilar'].isnull().sum()
missing_tedavi_after = df_processed['TedaviAdi'].isnull().sum()
print(f"missing_tani_after={missing_tani_after}, missing_tedavi_after={missing_tedavi_after}")

df_processed.to_excel('df_processed.xlsx', index=False)

# kan grubu HastaNo verisi odaklı grupalnıyor ve dolduruluyor daha sonra OHE
missing_before = df_processed['KanGrubu'].isnull().sum()
print(f"missing_before: {missing_before}")

df_processed['KanGrubu'] = df_processed.groupby('HastaNo')['KanGrubu'].transform(lambda x: x.ffill().bfill().infer_objects(copy=False))

missing_after = df_processed['KanGrubu'].isnull().sum()
print(f"missing_after: {missing_after}")


df_processed['KanGrubu'] = df_processed['KanGrubu'].fillna('Bilinmiyor')

threshold = 20
value_counts = df_processed['KanGrubu'].value_counts()
to_replace = value_counts[(value_counts < threshold) & (value_counts.index != 'Bilinmiyor')].index

df_processed['KanGrubu'] = df_processed['KanGrubu'].replace(to_replace, 'Bilinmiyor')

print("\n'Blood Group' value frequencies after merging:")
print(df_processed['KanGrubu'].value_counts())

kan_grubu_dummies = pd.get_dummies(df_processed['KanGrubu'], prefix='KanGrubu', dtype=int)

df_processed = pd.concat([df_processed, kan_grubu_dummies], axis=1)
df_processed.drop('KanGrubu', axis=1, inplace=True)

new_blood_cols = [col for col in df_processed.columns if col.startswith('KanGrubu_')]
print(f"\n new cols: {new_blood_cols}")

display(df_processed[['HastaNo'] + new_blood_cols].head())
df_processed.to_excel('df_processed.xlsx', index=False)

# sadece 1 tane doldurdu işe yaramadı
missing_before = df_processed['KronikHastalik'].isnull().sum()
print(f"before: {missing_before}")

df_processed['KronikHastalik'] = df_processed.groupby('HastaNo')['KronikHastalik'].transform(lambda x: x.ffill().bfill())

missing_after = df_processed['KronikHastalik'].isnull().sum()
print(f"after: {missing_after}")

print(f"{missing_before - missing_after} data filled.")

remaining_missing = df_processed['KronikHastalik'].isnull().sum()
print(f"\n current {remaining_missing} data is missing.")

display(df_processed.head())
display(df_processed.info())

#  Cinsiyet_Erkek binary klonu oluşturarak tek kolonla cinsiyet belirlendi.
df_processed['Cinsiyet'] = df_processed.groupby('HastaNo')['Cinsiyet'].transform(lambda x: x.ffill().bfill())

if df_processed['Cinsiyet'].isnull().any():
    cinsiyet_modu = df_processed['Cinsiyet'].mode()[0]
    df_processed['Cinsiyet'].fillna(cinsiyet_modu, inplace=True)

df_processed['Cinsiyet_Erkek'] = df_processed['Cinsiyet'].map({'erkek': 1, 'kadın': 0})

df_processed.drop('Cinsiyet', axis=1, inplace=True)
print(df_processed['Cinsiyet_Erkek'].value_counts())

missing_before = df_processed['UygulamaYerleri'].isnull().sum()
print(f"missing_before: {missing_before}")
# Her 'TedaviAdi' grubunun modunu hesaplayıp grupluanıyor.
# Bu, işlemi çok daha verimli hale getirir.
imputation_map = df_processed.groupby('TedaviAdi')['UygulamaYerleri'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)

print(imputation_map.head())

missing_idx = df_processed[df_processed['UygulamaYerleri'].isnull()].index

tedavi_adi_for_missing = df_processed.loc[missing_idx, 'TedaviAdi']

imputed_values = tedavi_adi_for_missing.map(imputation_map)

df_processed.loc[missing_idx, 'UygulamaYerleri'] = imputed_values

missing_after = df_processed['UygulamaYerleri'].isnull().sum()

print(f"\n missing_after: {missing_after}")

if df_processed['UygulamaYerleri'].isnull().any():
    # kalanları genel mod ile dolduruyorum.
    general_mode = df_processed['UygulamaYerleri'].mode()[0]
    df_processed['UygulamaYerleri'].fillna(general_mode, inplace=True)
    print(f"Nihai eksik değer sayısı: {df_processed['UygulamaYerleri'].isnull().sum()}")

# fill by mode
bolum_modu = df_processed['Bolum'].mode()[0]
df_processed['Bolum'].fillna(bolum_modu, inplace=True)

value_counts = df_processed['Bolum'].value_counts()
ana_kategori_1 = value_counts.index[0]
ana_kategori_2 = value_counts.index[1]

# iki kategori yeterli çünkü verinin %80'i FizikselTip, %10'u Ortopedi. Eğer ikiside 0 ise diğer kategoriler anlamına geliyor zaten.
df_processed['Bolum_Is_FizikselTip'] = np.where(df_processed['Bolum'] == ana_kategori_1, 1, 0)
df_processed['Bolum_Is_Ortopedi'] = np.where(df_processed['Bolum'] == ana_kategori_2, 1, 0)

df_processed.drop('Bolum', axis=1, inplace=True)

display(df_processed[df_processed['Bolum_Is_FizikselTip'] == 1][['Bolum_Is_FizikselTip', 'Bolum_Is_Ortopedi']].head(2)) # 1 0
display(df_processed[df_processed['Bolum_Is_Ortopedi'] == 1][['Bolum_Is_FizikselTip', 'Bolum_Is_Ortopedi']].head(2)) # 0 1
display(df_processed[(df_processed['Bolum_Is_FizikselTip'] == 0) & (df_processed['Bolum_Is_Ortopedi'] == 0)][['Bolum_Is_FizikselTip', 'Bolum_Is_Ortopedi']].head(2)) # 0 0
df_processed.to_excel('df_processed.xlsx', index=False)

"""## Feature engineering"""

# KronikHastalik_Sayisi - TedaviSüresine muhtemelen büyük etkisi vardir.
def count_chronic_diseases(disease_string):
    # kolonda 'Yok' verisi var
    if not isinstance(disease_string, str) or 'Yok' in disease_string:
        return 0
    else:
        diseases = disease_string.split(',')
        return len(diseases)

df_processed['KronikHastalik_Sayisi'] = df_processed['KronikHastalik'].apply(count_chronic_diseases)
display(df_processed[['HastaNo', 'KronikHastalik', 'KronikHastalik_Sayisi']].head(10))

# Tani_Ciddiyeti_Yuksek

# AI benim için hazırladı listeyi.
severity_keywords = [
    'kırık', 'kırığı',        # Fracture
    'rüptür', 'rüptürü',      # Rupture
    'yırtık', 'yırtığı',      # Tear
    'op', 'ameliyat', 'cerrahi',# Operation / Surgery
    'implant', 'protezi',     # Implant / Prosthesis
    'artroskopi',             # Arthroscopy (Surgical procedure)
    'menisektomi',            # Meniscectomy (Surgical procedure)
    'çıkık', 'çıkığı',        # Dislocation
    'hemipleji', 'parapleji',  # Paralysis
    'parezi',                 # Paresis (Weakness, often severe)
    'felç',                   # Paralysis
    'lezyon', 'lezyonu',      # Lesion
    'kanaması',               # Hemorrhage
    'yaralanma',              # Injury
    'sendromu'                # Sendromlar genellikle daha karmaşık durumları ifade eder
]

pattern = '|'.join(severity_keywords)
# hem tanilarda hem tedavi adinda arama yapıyorum
search_space = df_processed['Tanilar'].fillna('') + ' ' + df_processed['TedaviAdi'].fillna('')

df_processed['Tani_Ciddiyeti_Yuksek'] = search_space.str.contains(pattern, case=False, regex=True).astype(int)

display(df_processed[df_processed['Tani_Ciddiyeti_Yuksek'] == 1][['Tanilar', 'TedaviAdi', 'Tani_Ciddiyeti_Yuksek']].head())
display(df_processed[df_processed['Tani_Ciddiyeti_Yuksek'] == 0][['Tanilar', 'TedaviAdi', 'Tani_Ciddiyeti_Yuksek']].head())

# Ortopedi_ve_Ciddi_Tani
df_processed['Ortopedi_ve_Ciddi_Tani'] = ((df_processed['Bolum_Is_Ortopedi'] == 1) & (df_processed['Tani_Ciddiyeti_Yuksek'] == 1)).astype(int)
display(df_processed[df_processed['Ortopedi_ve_Ciddi_Tani'] == 1][['Bolum_Is_Ortopedi', 'Tani_Ciddiyeti_Yuksek', 'Ortopedi_ve_Ciddi_Tani']].head())
display(df_processed[(df_processed['Bolum_Is_Ortopedi'] == 1) & (df_processed['Tani_Ciddiyeti_Yuksek'] == 0)][['Bolum_Is_Ortopedi', 'Tani_Ciddiyeti_Yuksek', 'Ortopedi_ve_Ciddi_Tani']].head())

# Genel_Saglik_Skoru

risk_weights = {
    'duchenne': 3,
    'becker': 3,
    'limb-girdle': 3,
    'fascioscapulohumeral': 3,
    'polimiyozit': 3,
    'myastenia gravis': 3,
    'kalp yetmezliği': 3,

    'hipertansiyon': 2,
    'diyabet': 2,
    'aritmi': 2,

    'astım': 1,
    'hiportiroidizm': 1,
    'hipertiroidizm': 1,
    'guatr': 1
}

def calculate_health_score(disease_string):
    if pd.isna(disease_string) or 'yok' in disease_string.lower():
        return 0

    total_score = 0
    for keyword, weight in risk_weights.items():
        if keyword in disease_string:
            total_score += weight

    return total_score

df_processed['Genel_Saglik_Skoru'] = df_processed['KronikHastalik'].apply(calculate_health_score)

display_cols = ['HastaNo', 'KronikHastalik', 'Genel_Saglik_Skoru']
display(df_processed[display_cols].head(4))
print(df_processed['Genel_Saglik_Skoru'].describe())
print(df_processed['Genel_Saglik_Skoru'].value_counts().sort_index())

# çoklu ilac alerji kolonu
medicine_allergy_colons = [
    'Alerji_arveles',
    'Alerji_coraspın',
    'Alerji_gripin',
    'Alerji_novalgin',
    'Alerji_voltaren'
]

current_medicine_allergies = [col for col in medicine_allergy_colons if col in df_processed.columns]
df_processed['Coklu_Ilac_Alerjisi_Sayisi(Polifarmasi)'] = df_processed[current_medicine_allergies].sum(axis=1)

display_cols = ['HastaNo'] + current_medicine_allergies + ['Coklu_Ilac_Alerjisi_Sayisi(Polifarmasi)']
display(df_processed[df_processed['Coklu_Ilac_Alerjisi_Sayisi(Polifarmasi)'] > 0][display_cols].head(10))
print(df_processed['Coklu_Ilac_Alerjisi_Sayisi(Polifarmasi)'].value_counts().sort_index())
df_processed.to_excel('df_processed.xlsx', index=False)

# Tani_Sayisi
def count_diagnoses(diagnosis_string):
  if pd.isna(diagnosis_string) or not diagnosis_string.strip():
      return 0
  else:
      diagnoses = str(diagnosis_string).strip().split(',')
      return len(diagnoses)

df_processed['Tani_Sayisi'] = df_processed['Tanilar'].apply(count_diagnoses)

filtered_df = df_processed[df_processed['Tani_Sayisi'] > 1]
display_cols = ['HastaNo', 'Tanilar', 'Tani_Sayisi']
display(filtered_df[display_cols].head(10))

# Tedavi_Odak_Bolgesi. Tedavi bölgesi tedavi süresine doğrudan etki eden bir veri. Bu nedenle daha kolay kategorizasyon için bu şekilde sınıflandırmak istedim.
def get_body_part_group(location_string):
    if pd.isna(location_string):
        return 'Bilinmiyor'

    location_lower = str(location_string).lower()

    if any(keyword in location_lower for keyword in ['omuz', 'dirsek', 'el', 'kol', 'parmak']):
        return 'Ust_Ekstremite'
    elif any(keyword in location_lower for keyword in ['kalça', 'diz', 'ayak', 'bacak', 'femur', 'aşil', 'topuk', 'malleol']):
        return 'Alt_Ekstremite'
    elif any(keyword in location_lower for keyword in ['boyun', 'sırt', 'bel', 'gövde', 'vücut', 'dorsal', 'koksiks', 'torasik']):
        return 'Govde'
    else:
        return 'Diger_Bolge'

df_processed['Tedavi_Odak_Bolgesi'] = df_processed['UygulamaYerleri'].apply(get_body_part_group)
odak_bolgesi_dummies = pd.get_dummies(df_processed['Tedavi_Odak_Bolgesi'], prefix='OdakBolgesi', dtype=int)
df_processed = pd.concat([df_processed, odak_bolgesi_dummies], axis=1)
df_processed.drop('Tedavi_Odak_Bolgesi', axis=1, inplace=True)

yeni_sutunlar = [col for col in df_processed.columns if col.startswith('OdakBolgesi_')]
display_cols = ['UygulamaYerleri'] + yeni_sutunlar

display(df_processed[display_cols].sample(10))

# mevcut normal yaş verisi ile 40-10=30 70-40=30 gibi birbiriyle aynı durum var(bilgisayar için)
# ama gerçek hayatta 10 yaşından 40 yaşına geçişte çok büyük bir sağlık düşüşü olmaz, ama 40 yaşından 70 yaşına büyük bir düşüş olur(genellikle)
# yani yaş büyüdükçe sağlık kötüleşme hızıda büyür bu nedenle yaşın^2 verisi hayatın gerçeklerini daha iyi yansıtır diye düşündüm.
df_processed['Yas_Karesi'] = df_processed['Yas'] ** 2

# tedavi süresinde yaşın çok etkis olacağını düşündüğüm için aynı zamanda One Hot Encoding ile kategorik bir sütun oluşturdum.
yas_bins = [1, 5, 12, 18, 40, 65, np.inf] # en küçük yaş 2
yas_labels = [
    '0_Bebek (2-4)',
    '1_Cocuk (5-11)',
    '2_Ergen (12-17)',
    '3_Genc_Yetiskin (18-39)',
    '4_Orta_Yas (40-64)',
    '5_Yasli (65+)'
]
df_processed['Yas_Grubu'] = pd.cut(df_processed['Yas'], bins=yas_bins, labels=yas_labels, right=True)
yas_grubu_dummies = pd.get_dummies(df_processed['Yas_Grubu'], prefix='YasGrubu', dtype=int)
df_processed = pd.concat([df_processed, yas_grubu_dummies], axis=1)
df_processed.drop('Yas_Grubu', axis=1, inplace=True)

yeni_yas_sutunlari = ['Yas', 'Yas_Karesi'] + [col for col in df_processed.columns if col.startswith('YasGrubu_')]

display(df_processed[yeni_yas_sutunlari].sample(5))
df_processed.to_excel('df_processed.xlsx', index=False)

"""## Frequency Encoding"""

generate_comprehensive_report(df_processed)

def frequency_encoding(df, column_name, new_col_name):
    print(f"'{column_name}' sütunu için Gelişmiş Frekans Kodlaması yapılıyor...")

    freq_map = df[column_name].value_counts()
    rank_map = freq_map.rank(method='first', ascending=False)
    total_categories = len(freq_map)
    final_score_map = freq_map + (1 - rank_map / total_categories)

    df[new_col_name] = df[column_name].map(final_score_map)
    df[new_col_name].fillna(0, inplace=True)

    print(f" -> '{new_col_name}' sütunu oluşturuldu.")
    return df


# "sağ/sol" temizliği
df_processed['UygulamaYerleri_cleaned'] = df_processed['UygulamaYerleri'].str.replace(r'\b(sağ|sol)\b', '', regex=True, flags=re.IGNORECASE)
df_processed['UygulamaYerleri_cleaned'] = df_processed['UygulamaYerleri_cleaned'].str.replace(r'\s+', ' ', regex=True).str.strip()
df_processed = frequency_encoding(df_processed, 'UygulamaYerleri_cleaned', 'UygulamaYerleri_AdvFreq')


# Nadir kategorileri gruplama
threshold = 5
value_counts = df_processed['TedaviAdi'].value_counts()
to_replace = value_counts[value_counts < threshold].index
df_processed['TedaviAdi_grouped'] = df_processed['TedaviAdi'].replace(to_replace, 'Diger_Tedavi')
df_processed = frequency_encoding(df_processed, 'TedaviAdi_grouped', 'TedaviAdi_AdvFreq')

# ',' kullanıldığı için ilk değerin frekansını atıyorum
df_processed['Ana_KronikHastalik'] = df_processed['KronikHastalik'].str.split(',').str[0].str.strip()
df_processed = frequency_encoding(df_processed, 'Ana_KronikHastalik', 'KronikHastalik_AdvFreq')

df_processed['Ana_Tani'] = df_processed['Tanilar'].str.split(',').str[0].str.strip()
df_processed = frequency_encoding(df_processed, 'Ana_Tani', 'Tanilar_AdvFreq')


columns_to_drop = [
    'UygulamaYerleri', 'UygulamaYerleri_cleaned',
    'TedaviAdi', 'TedaviAdi_grouped',
    'KronikHastalik', 'Ana_KronikHastalik',
    'Tanilar', 'Ana_Tani'
]

df_processed.drop(columns_to_drop, axis=1, inplace=True)


# Oluşturulan yeni frekans sütunları
freq_cols = [col for col in df_processed.columns if 'AdvFreq' in col]

print("\nOluşturulan yeni frekans sütunları ve ilk 5 satırları:")
display(df_processed[freq_cols].head())

print("\nNihai DataFrame'in boyutu:", df_processed.shape)

generate_comprehensive_report(df_processed)

"""## **Hedef Veri**

TedaviSuresi'nin %70'inden fazlası tam olarak 15 seanslık bir tedavi paketine atanmış. Geri kalanlar da 10 seans 20 seans gibi belirli sayılarda toplanmıştı.
Yani aslında bu bir *`seans sayı`* tahmini değil bir *`seans paketi`* tahmini olacak.
Yani bir regresyon modeli değil çok sınıflı bir `sınıflandırma modeline` göre veri hazırlanmalı.
"""

plt.figure(figsize=(10, 6))
sns.countplot(x='TedaviSuresi', data=df_processed, palette='viridis')
plt.title("Yeni Hedef Değişkenin (Kategorik) Dağılımı")
plt.xlabel("Tedavi Süresi Kategorisi")
plt.ylabel("Hasta Sayısı")
plt.show()

bins = [
    0,
    8,
    14,
    15,
    np.inf
]

labels = [
    '0_Cok_Kisa',
    '1_Orta',
    '2_Standart_15',
    '3_Uzun'
]

df_processed['TedaviSuresi_Kategori'] = pd.cut(df_processed['TedaviSuresi'],
                                              bins=bins,
                                              labels=labels,
                                              right=True)

df_processed['y_target_sinif'] = df_processed['TedaviSuresi_Kategori'].cat.codes

print(df_processed['TedaviSuresi_Kategori'].value_counts().sort_index())

display(df_processed[['TedaviSuresi', 'TedaviSuresi_Kategori', 'y_target_sinif']].sample(10))

# Yeni hedef değişkenimizin dağılımını görselleştirelim
plt.figure(figsize=(10, 6))
sns.countplot(x='TedaviSuresi_Kategori', data=df_processed, order=labels, palette='viridis')
plt.title("Yeni Hedef Değişkenin (Kategorik) Dağılımı")
plt.xlabel("Tedavi Süresi Kategorisi")
plt.ylabel("Hasta Sayısı")
plt.show()

"""## StandardScaler"""

from sklearn.preprocessing import StandardScaler

y_target = df_processed['y_target_sinif']
non_features = ['HastaNo', 'TedaviSuresi', 'y_target_sinif', 'KronikHastalik', 'Tanilar', 'UygulamaYerleri', 'TedaviAdi']
features_df = df_processed.drop(columns=[col for col in non_features if col in df_processed.columns])


# Ölçeklendirilecek sütunlar
cols_to_scale = [
    'Yas', 'UygulamaSuresi', 'KronikHastalik_Sayisi', 'Genel_Saglik_Skoru',
    'Tani_Sayisi', 'Yas_Karesi', 'UygulamaYerleri_cleaned_AdvFreq',
    'TedaviAdi_grouped_AdvFreq'
]
# Sadece df'te var olanları seç
cols_to_scale = [col for col in cols_to_scale if col in features_df.columns]


# Ölçeklendirilmeyecek (binary) sütunlar
cols_not_to_scale = [col for col in features_df.columns if col not in cols_to_scale]


scaler = StandardScaler()

scaled_data = scaler.fit_transform(features_df[cols_to_scale])

df_scaled = pd.DataFrame(scaled_data, columns=cols_to_scale)

print("Seçilen sütunlar başarıyla ölçeklendirildi.")

df_not_scaled = features_df[cols_not_to_scale].reset_index(drop=True)

# Ölçeklenmiş ve ölçeklenmemiş özellikleri birleş
X_final = pd.concat([df_scaled, df_not_scaled], axis=1)

display(X_final[cols_to_scale].describe().T)

generate_comprehensive_report(df_processed)

import pandas as pd
import numpy as np

final_df = X_final.copy()


print("--- Nihai DataFrame'i Yeniden Sıralama ve İsimlendirme ---")

demografik_ozellikler = [
    'Yas',
    'Yas_Karesi',
    'Cinsiyet_Erkek',
    'YasGrubu_0_Bebek (2-4)',
    'YasGrubu_1_Cocuk (5-11)',
    'YasGrubu_2_Ergen (12-17)',
    'YasGrubu_3_Genc_Yetiskin (18-39)',
    'YasGrubu_4_Orta_Yas (40-64)',
    'YasGrubu_5_Yasli (65+)'
]

tibbi_gecmis_ozellikleri = [
    'Genel_Saglik_Skoru',
    'KronikHastalik_Sayisi',
    'KanGrubu_Bilinmiyor',
    'KanGrubu_0 rh+',
    'KanGrubu_a rh+',
    'KanGrubu_b rh+',
    'KanGrubu_ab rh+',
    'KanGrubu_b rh-',
]

alerji_ozellikleri = [
    'Coklu_Ilac_Alerjisi_Sayisi',
    'Alerji_Alerji_Yok',
    'Alerji_polen',
    'Alerji_toz',
    'Alerji_novalgin',
    'Alerji_sucuk',
    'Alerji_voltaren',
    'Alerji_arveles',
    'Alerji_coraspın',
    'Alerji_gripin',
    'Alerji_yer fıstığı'
]

tani_ve_tedavi_ozellikleri = [
    'UygulamaSuresi',
    'Bolum_Is_FizikselTip',
    'Bolum_Is_Ortopedi',
    'Tani_Sayisi',
    'Tani_Ciddiyeti_Yuksek',
    'Ortopedi_ve_Ciddi_Tani',
    'OdakBolgesi_Govde',
    'OdakBolgesi_Ust_Ekstremite',
    'OdakBolgesi_Alt_Ekstremite',
    'OdakBolgesi_Diger_Bolge',
    'TedaviAdi_grouped_AdvFreq',
    'UygulamaYerleri_cleaned_AdvFreq'
]

tum_sutunlar = demografik_ozellikler + tibbi_gecmis_ozellikleri + alerji_ozellikleri + tani_ve_tedavi_ozellikleri
mevcut_sutunlar = [col for col in tum_sutunlar if col in final_df.columns]

final_df = final_df[mevcut_sutunlar]

print("Adım 1: Sütunlar mantıksal gruplara göre başarıyla yeniden sıralandı.")

rename_dict = {
    'TedaviSuresi': 'TedaviSuresi (seans)',
    'UygulamaSuresi': 'UygulamaSuresi (dakika)'
}

if 'UygulamaSuresi' in final_df.columns:
    final_df.rename(columns={'UygulamaSuresi': 'UygulamaSuresi (dakika)'}, inplace=True)
    print("Adım 2: 'UygulamaSuresi' sütunu 'UygulamaSuresi (dakika)' olarak yeniden adlandırıldı.")
else:
    print("Adım 2: 'UygulamaSuresi' sütunu bulunamadığı için yeniden adlandırma yapılmadı.")

display(final_df.head())

final_df.to_excel('Talent_final_bitmis_data.xlsx', index=False)
y_target.to_excel('y_target_bitmis_hedef_datası.xlsx', index=False)

"""# Test"""

import pandas as pd
from sklearn.model_selection import train_test_split

X = pd.read_excel('Talent_final_bitmis_data.xlsx')
y = pd.read_excel('y_target_bitmis_hedef_datası.xlsx').iloc[:, 0]

# 2. %80 train, %20 test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train.to_excel('X_train.xlsx', index=False)
X_test.to_excel('X_test.xlsx', index=False)
y_train.to_excel('y_train.xlsx', index=False, header=True)
y_test.to_excel('y_test.xlsx', index=False, header=True)

import pandas as pd
import numpy as np
import torch
from torch import nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Kullanılan Cihaz: {device}")

X_train_df = pd.read_excel('X_train.xlsx')
X_test_df = pd.read_excel('X_test.xlsx')
y_train_df = pd.read_excel('y_train.xlsx')
y_test_df = pd.read_excel('y_test.xlsx')

# DataFrame'leri NumPy array'lerine çeviriyoruz
X_train_np = X_train_df.values
X_test_np = X_test_df.values
y_train_np = y_train_df.values.flatten()
y_test_np = y_test_df.values.flatten()

# NumPy array'lerini PyTorch tensor'larına dönüştürüyoruz
# Özellikler float, etiketler long (integer) olmalıdır.
X_train = torch.from_numpy(X_train_np).float()
X_test = torch.from_numpy(X_test_np).float()
y_train = torch.from_numpy(y_train_np).long()
y_test = torch.from_numpy(y_test_np).long()

if X_train is not None:
    INPUT_FEATURES = X_train.shape[1]
    OUTPUT_FEATURES = len(torch.unique(y_train))
    HIDDEN_UNITS = 128

    model = nn.Sequential(
        nn.Linear(in_features=INPUT_FEATURES, out_features=HIDDEN_UNITS),
        nn.ReLU(),
        nn.Linear(in_features=HIDDEN_UNITS, out_features=HIDDEN_UNITS // 2),
        nn.ReLU(),
        nn.Linear(in_features=HIDDEN_UNITS // 2, out_features=OUTPUT_FEATURES)
    ).to(device)

    print("\nModel Mimarisi:")
    print(model)

    # Çok sınıflı sınıflandırma için CrossEntropyLoss
    loss_fn = nn.CrossEntropyLoss()

    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)

    torch.manual_seed(42)
    torch.cuda.manual_seed(42)

    epochs = 450

    X_train, y_train = X_train.to(device), y_train.to(device)
    X_test, y_test = X_test.to(device), y_test.to(device)

    print("\nEğitim döngüsü başlatılıyor...")
    for epoch in range(epochs):
        model.train()

        # 1. Forward pass
        y_logits = model(X_train)

        # 2. Loss hesaplama
        loss = loss_fn(y_logits, y_train)

        # 3. Optimizer'ın gradyanlarını sıfırlama
        optimizer.zero_grad()

        # 4. Geriye yayılım (Backpropagation)
        loss.backward()

        # 5. Ağırlıkları güncelleme
        optimizer.step()

        # Değerlendirme
        model.eval()
        with torch.inference_mode():
            test_logits = model(X_test)
            test_loss = loss_fn(test_logits, y_test)

        # Her 20 epoch'ta bir durumu yazdır
        if epoch % 25 == 0:
            # Doğruluk hesaplama
            y_pred_class = torch.softmax(y_logits, dim=1).argmax(dim=1)
            train_acc = (y_pred_class == y_train).sum().item() / len(y_train) * 100

            test_pred_class = torch.softmax(test_logits, dim=1).argmax(dim=1)
            test_acc = (test_pred_class == y_test).sum().item() / len(y_test) * 100

            print(f"Epoch: {epoch:4d} | Train Loss: {loss:.5f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%")

    print("\n--- Final Model Performans Değerlendirmesi ---")
    model.eval()
    with torch.inference_mode():
        # Test seti üzerinde nihai tahminleri yap
        y_preds_logits = model(X_test)
        # Logitleri sınıflara çevir (0, 1, 2, 3)
        y_preds = torch.softmax(y_preds_logits, dim=1).argmax(dim=1)

    # Tensor'ları CPU'ya geri alıp NumPy'e çevirerek scikit-learn raporlarını kullanabiliriz.
    y_preds_np = y_preds.cpu().numpy()
    y_test_np = y_test.cpu().numpy()

    # Sınıf etiketleri
    class_labels = ['0_Cok_Kisa', '1_Orta', '2_Standart_15', '3_Uzun']

    print("\nSINIFLANDIRMA RAPORU:")
    print(classification_report(y_test_np, y_preds_np, target_names=class_labels))